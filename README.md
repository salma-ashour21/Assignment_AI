# Assignment_AI
Gradient Descent – Updates weights by moving in the direction that reduces the loss.

Stochastic Gradient Descent (SGD) – Updates weights using one sample at a time for faster training.

Mini-Batch Gradient Descent – Uses small batches of data to update weights efficiently.

Momentum – Adds a velocity term to speed up learning and reduce oscillations.

Nesterov Accelerated Gradient (NAG) – Looks ahead before updating to make more accurate moves.

Adagrad – Adjusts the learning rate individually for each parameter based on frequency.

RMSprop – Keeps a moving average of squared gradients to control learning rate.

Adam (Adaptive Moment Estimation) – Combines Momentum and RMSprop for stable and fast convergence.

AdaDelta – Improves Adagrad by preventing the learning rate from decreasing too much.

AdamW – A variant of Adam that improves generalization by decoupling weight decay.
